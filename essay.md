# Реферат

## 1. Вступление

Запись и повторное исполнение программы является довольно важным, а главное удобным инструментом для тестирования и отладки. На данный момент существует много подходов для его реализации, но все они имеют недостатки: где-то требуется сохранять состояние всей виртуальной машины, где-то нужно модифицировать ядро ОС или же компиляторы, в результате снижая область применения или же увеличивая накладные расходы. Важно понять, как это сделать с малым снижением или вовсе без потерь в производительности с учетом того, чтобы отлаживаемая программа работала абсолютно идентично той, что была записана на определенном устройстве, на всех поддерживаемых устройствах.

Здесь будет рассматриваться подход к разработке системы RR записи и повторного воспроизведения программы без изменения пользовательского пространства приложения, со стандартным набором инструментов Linux и базируясь на x86/x86_64 процессорах.

## 2. Дизайн

Основные используемые концепции:

- Границы состояний и вычислений для поддержания детерминизма.

- Сохранение почти всего пользовательского пространства и запуск некоторой группы процессов данного пространства.

- Лишь самая необходимая часть состояний ядра сохраняется (например, вместо выполнения самих операций — сохраняется только результат самой операции).

- Создается идентичная модель разметки памяти. На один отслеживаемый процесс, адрес памяти, поток, создается идентичный отслеживаемый объект, нужный для повторного исполнения.

Рассмотрим, как они используются в решении некоторых задач эффективности системы записи и повторного исполнения программы.

### 2.1 Избегание гонок данных

Сама по себе гонка данных (одновременный доступ нескольких потоков к одной и той же области памяти) — недетерминированное событие. Решение — запускать лишь один поток в единицу времени и самому контролировать переключение между ними.

Условие одного потока в единицу времени реализуется за счет взаимодействий с системным вызовов *ptrace*. Представим, RR имеет определенный набор наблюдаемых потоков, то есть используя данный системный вызов, он контролирует их исполнение и имеет доступ к его информации (состояние, адрес памяти, состояние регистров и т.п.). Если один из наблюдаемых потоков начинает исполнять системный вызов, RR получает сигнал и решает, нужно ли выполнить смену контекста и начать исполнять кого-то другого или нет. Если да (особенно если это был блокирующий системный вызов), то для текущего потока посылается *PTRACE_SYSCALL*, он исполняется до тех пор, пока не выйдет из системного вызова, останавливается, и для уже выбранного потока посылается любой валидный сигнал, говорящий продолжать работу. Если же какой-то поток наткнулся на точку предварительно записанной смены контекста, то RR запрашивает текущее состояние всех наблюдаемых потоков (используя *waitpid*), чтобы найти кандидатов на продолжение работы. Выбор между готовыми кандитатами работает через приоритеты, а в случае потоков с одинаковыми приоритетами — последовательно по кругу.

В целом, подобное решение гораздо проще реализуемое, чем, например, поддержание целой виртуальной среды исполнения. Оно не следует ложному представлению того, что программы не имеют гонок, и оно тем эффективнее, чем меньше параллелизма существует в отлаживаемой системе. Конечно, при высоком параллелизме уровень задержки серьезно вырастает, но эта проблема решается ограничиванием рабочего окружения и связанных с ним объемом данных.

### 2.2 Системные вызовы

Ptrace дает хорошую вариативность в управлении пользовательских данных для наблюдаемого потока, а также в контроле исполнений системных вызовов внутри него. Когда поток входит или выходит из системного вызова, RR, как наблюдатель, уведомляется об этом. Это работает так, что при входе в системный вызов поток останавливается и RR уведомляется, и когда RR решает возобновить поток, то этот поток продолжает исполнение до тех пор, пока не заканчивает исполнение системного вызова, и уведомляет RR еще раз. Это удобно тем, что во время уведомлений можно сохранить и даже изменить состояние пользовательских данных.

#### 2.2.1 Рабочий буфер

Представим ситуацию, что у нас поток, остановившийся в результате входа в блокирующий системный вызов. В этот момент, следуя концепции исполнения одного потока в единицу времени, RR начинает исполнять другой поток. И в этот момент есть вероятность что работающий в данный момент поток добрался до выходного буфера системного вызова остановившегося потока, в результате чего возникает гонка данных с ядром, который пишет в этот буфер. Решение — сделать рабочую буфер, в который перенаправляются все данные из выходного буфера на момент исполнения другого потока, и, после выхода из системного вызова и перехода в соответствующий поток, возвратить в исходный буфер из рабочего.

#### 2.2.2 Эмуляция Ptrace

Могут возникнуть ситуации помимо отладки, когда программам нужно воспользоваться системным вызовов ptrace (собрать данные после краша программы и т.п.). В целом, Linux не позволяет иметь рекурсивную цепочку отношений наблюдатель-наблюдаемый, поэтому, если наблюдаемый процесс вызывает ptrace, то он выполнится с ошибкой. То есть никакой процесс, отлаживаемый через RR, не смог бы вызвать его. Однако, RR может эмулировать все операции и отношения внутри ptrace, несмотря на сложность.

#### 2.2.3 Выполнение системных вызовов в контексте наблюдаемого потока

Иногда нужно выполнять системные вызовы в контексте наблюдаемого потока, и здесь, также с помощью ptrace, допускается вариативность. Самый непростой подобный сценарий — это когда нужно исполнить системный вызов с заранее определенными параметрами. В таком случае RR просто определяет местоположение необходимых параметров для нужного системного вызова, временно подменяет их (допустимо как для параметров на стеке, так и в крайнем случае для параметров в адресном пространстве) и возвращает в исходное состояние как только системный вызов выполнится.

#### 2.2.4 Пространство имен ядра

Так называемое пространство имен ядра (Kernel Namespaces) — полезная особенность, помогающая с реализацией "контейнеров" и изолированных (sandboxing) приложений. В случае RR иногда важно взаимодействовать с подобными приложениями. Для этого резервируется файловый дескриптор во всех наблюдаемых процессах, ссылающийся на корневую директорию, также видимую для RR.

Это можно расценивать как уязвимость, поскольку, если наблюдаемый поток знает, что он изолирован, то он сможет использовать это в своих интересах. ПОэтому предполагается, что все наблюдаемые потоки об этом не знают.

#### 2.2.5 Seccomp

Seccomp — это одна из возможностей в системе Linux, заключающаяся в фильтрации системных вызовов. В ядро устанавливается фильтр *seccomp-bpf*, определяющий вариант исполнения какого-либо системного вызова (завершится ли он нормально, с ошибкой, выкинет ли сигнал или ptrace trap). Важно помнить, что оно само по себе не является средством изоляции, но помогает изолированным процессам в контроле "открытости" ядра.

RR немного расширяет политику "seccomp" на множество своих собственных системных вызовов. Всегда, после старта программы внутри наблюдаемого потока, RR аллоцирует новую страницу в адресном пространстве потока, "RR Page", в которой хранятся все "привилегированные" системные вызовы, на основе которой формируется байт-код их фильтрации. Далее, исследуется код бинарника, который запускается на наблюдаемом потоке, на предмет системных вызовов, отвечающих за установку *seccomp-bpf* фильтров. Если таковы были найдены, то в пролог этой функции дополнительно отправляется сформированный ранее байт-код фильтрации, и, если в процессе исполнения функции поток натыкается на "привилегированный" системный вызов, то он отравляет сигнал для RR с параметром "allow". Эти "привилегированные" системные вызовы используются, когда надо выполнить системные вызовы, определенные в RR, в контексте наблюдаемого потока.

Также записывается и стандартное поведение *seccomp-bpf*, то есть на стандартных системных вызовах. Для этого просто исследуется поведение ядра в случае исполнения данного системного вызова. Это очевидный и простой подход, избавляющий от необходимости писать свой собственный интерпретатор BPF в RR.

#### 2.2.6 Моделирование системных вызовов на практике

Из концепции границ состояний, предполагается, что можно эффективно определить состояние данных перед и после системного вызова. Другими словами, поведение любого системного вызова должно быть детерминированным. На практике это не всегда так.

Например, системный вызов ioctl. Его проблема в том, что его поведение зависит от ассоциированного с ним I/O устройства. В таком случае, его модель строится на основе большого количества вариантов его поведения. Такой подход требует время от времени поддержку все новых I/O устройств.

#### 2.2.7 Повторное исполнение

При повторном исполнении программы есть еще один вариант исполнения системных вызовов. Вместо использования системного вызова ptrace с флагом *PTRACE_SYSCALL* или *PTRACE_SYSEMU* просто ставится точка останова (breakpoint) на момент входа в некоторый системный вызов. Это работает так, что программный счетчик (programme counter) наблюдаемого потока доходит до инструкции системного вызова, его исполнение останавливается. RR это понимает, и в этот момент подменяет данные на те, что должны получиться в результате исполнения этого системного вызова. RR знает о них, поскольку идет повторное исполнение программы.

Данный подход эффективнее, поскольку снижает количество смен контекста между наблюдаемым потоком и наблюдателем (RR) с 2 до 1, что довольно существенно. Однако это не всегда может работать корректно. Например, наблюдаемый поток сам напишет в память инструкцию системного вызова и после прыгнет туда, что может переписать программную точку останова (software breakpoint). Использование аппаратной точки останова (hardware breakpoint) тоже не вариант, так как из-за их ограниченного количества могут возникнуть конфликты с таковыми внутри наблюдаемого потока (то есть когда он, например, уже поставил все аппаратные точки останова). Следовательно, если системный вызов в разделяемой памяти или памяти с возможностью записи, то используется стандартный подход.

#### 2.2.8 Повторное исполнение сложных системных вызовов

Некоторые системные вызовы взаимодействуют с потоками или память, и с к каждому из них требуются собственный подход.

Для системного вызова *mmap* во время повторного исполнения нужно его вызывать с флагом *MAP_FIXED*, чтобы гарантировать, что страница будет аллоцирована так же, как и в первом исполнении.

Для системного вызова *execve* необходимо сохранить данные о том, как было размечено адресное пространство (по каким адресам выделились страницы), потому что из-за ASLR адресное пространство размещается рандомизированно. Сам процесс воссоздания той же разметки адресного пространства начинается с того, что во время повторного исполнения заместо настоящего *execve* используется сначала заглушка, которая ничего не аллоцирует, и уж потом аллоцируются страницы в памяти.

#### 2.2.9 Обработчики сигналов

Стандартная схема обработки сигнала такова: сигнал (синхронный или асинхронный) срабатывает, далее ядро настраивает фрейм стека пользовательского пространства под обработчик и передает управление по адресу этого обработчика. Во время записи (первого исполнения отлаживаемой программы) ptrace сообщает об этом сигнале, если этот сигнал отслеживается через ptrace, далее RR передает управление обработчику сигнала, используя *PTRACE_SINGLESTEP* флаг. Важно, что RR должен знать о всех существующих обработчиках сигнала, потому что в его власти передавать им управление. Это нужно затем, чтобы  потом, во время отладки, выбирать, какие обработчики и краевые случаи игнорировать (например, *SIGILL* или *SIGSEGV* сигналы), ведь наблюдаемый поток мог использовать *sigprocmask*.

Когда RR переходит в обработчик сигнала, он записывает содержимое фрейма стека и регистров, установленных ядром. Во время повторного исполнения, никакие обработчики не исполняются и никакие сигналы не доставляются по стандартной схеме. Тут как раз и нужны данные о записанных обработчиках сигналов. Чтобы воспроизвести начало исполнения некоторого обработчика, записывается его известный адрес в память, и регистры устанавливаются в нужные значения. Это помогает избежать необходимости ставить обработчики, то есть снижает количество работы.

#### 2.2.10 Прерывание системных вызовов сигналами

В Linux имеется два сценария обработки прерываний системных вызовов:

  1. Системный вызов прерывается и потом перезапускается
  2. Системный вызов прерывается, но после этого продолжается исполнение кода в стандартном режиме

Для RR важно понимать, с какой точки во время повторного исполнения продолжать исполнение кода после прерывания и соответственно как изменить регистры, стек, память и т.п. Если бы RR спутал повтор входа в системный вызов с новым вызовом, оно могло бы ошибочно записать или применить результат там, где на самом деле должен был быть перезапуск системного вызова, что в свою очередь приведет к несоответствию регистров и ошибке. Сложность также составляет ptrace, который не различает повторное исполнение системного вызова при уведомлении RR и исполнение последующего в коде. Поэтому нужные доп. структуры для отслеживания этого факта.

Когда системный вызов прерывается, ядро корректируют регистры (в особенности, программный счетчик) так, что тот указывает уже после системного вызова, но особыми служебными значениями в регистре результата (например, значение *ERESTARTSYS*, служащее для того, чтобы понять, требуется ли перезапустить системный вызов). Если оно стоит, то после возврата из обработчика через *sigreturn* и некоторых других условий, программный счетчик еще раз изменяется на значение равное адресу кода перед системным вызовом, что заставляет его перезапуститься.

Решение в том, чтобы иметь стек для эмулирования цепочки сценариев прерывания. Когда ядро фиксирует прерывание и переход в обработчик, RR делает об этом пометку в виде значений регистров при входе в системный вызов в своем стеке вызовов. Если после обработки поток хочет перезапустить системный вызов (RR понимает это по причинам выше), RR соотносит значения регистров, и если они совпадают, значит вызов хочет перезапуститься. Метка со стека просто снимается. Если же это не так и регистры не совпадают, значит ядро отбросило повторный перезапуск и направило поток идти дальше. В таком случае при первом исполнении изменения после исполнения системного вызова записываются, а при последующий просто применяются. Важно, что при повторном исполнении эмулируется вся цепочка перезапусков.

### 2.3 Асинхронные события

Есть два типа асинхронных событий, которые RR должен поддерживать: смена контекста и сигналы. Эмуляция их срабатываний будет происходить через специальную смену контекста между наблюдателем и наблюдаемым посредством отправки сигнала наблюдаемому потоку. Важно, чтобы во время повторного исполнения эти события срабатывали точно в тот же момент, в который они возникли во время записи, то есть когда сигнал был доставлен, состояние исполняющего потока должно быть идентичным.

Рассмотрим, что для этого было сделано.

#### 2.3.1 Недетерминированные счетчики производительности

Чтобы корректно симулировать моменты доставки сигналов, нужно корректно определять эти самые моменты, то есть, буквально, на какой инструкции вызывать это событие и чтобы при этом данные совпали. Проблема в том, что большинство данных, которые для этого поставляет процессор и которые можно использовать для определения инструкции, от одного исполнения к другому на одном и том же участке кода, используя одно и то же устройство, могут отличаться. Другими, словами, это данные недетерминированы. Все потому, что это связано с многими факторами, на которые эти данные опираются (архитектура процессора, содержание кэшей, состояние таблицы страниц, спекулятивное исполнение инструкций и т.п.).

Тем не менее, современные процессоры на базе Intel имеют поставщик детерминированных данных — счетчик производительности "количества отработанных ветвлений" (*RCB*). Все же, нельзя полагаться исключительно на него во время определения вызова события как минимум потому, что важна конкретная инструкция, после которой сработало событие во время записи. Следовательно, к счетчику добавляется еще записанное состояние регистров (в большинстве случаев достаточно регистров общего назначения).

Если рассмотреть ситуации, когда RR используется не только для записи и повторного исполнения, но и например для обратного исполнения, то можно понять, что даже знания RCB и состояния регистров общего назначения не гарантирует состояние программы на данный момент, потому что обратное исполнение в реализация RR выполняет частые остановки (исполнение по одному шагу) и на множественных сравнениях одной точки остановки с другой. Как это связано? Представим, функция А содержит множественные вызовы функцию Б. Ни в одной, ни в другой функции не возникают условные ветвления, и, вследствие оптимизаций, на какой-нибудь инструкции в функции Б регистры общего назначения будут иметь те же значения, что при вызове функции А. Решение заключается в том, чтобы вместе с записыванием регистров и RCB на каждой остановке собирать данные со стека.

Можно ли сделать иначе?

- Можно рассмотреть иные счетчики для решения данной задачи. Взять, например, счетчик отработавших переходов (branches retired counter), но он недетерминирован, потому что есть переходы (например, из-под аппаратного прерывания), которые видны процессору, но не видны пользователю и, соответственно, для RR тоже.

- Можно использовать виртуализацию, но проблема в том, что большинство средств виртуализации не реализуют в себе подобные счетчики. Это связано с тем, что если подобные счетчики "настоящие", то с их помощью одна виртуальная машина может наблюдать за другой, что противоречит концепции виртуализации. Если же полностью виртуализовать эти счетчики, то это существенно снизит производительность, да и в целом сведет сведет на нет весь смысл существования счетчиков производительности. Поэтому в большинстве виртуальных машин они не реализованы, из-за чего в них не работает и RR.

- Можно использовать счетчик отработанных инструкций (retired instructions), но он не всегда считает корректно. Например, если инструкцию после прерывания засчитывается дважды (из-за перезапуска, о котором говорилось выше), хотя в коде это не так. Тем более, он также учитывает инструкции, исполняемые в безопасном режиме, то есть те, которые не видимы для RR

- Можно забыть про счетчики производительности и сделать собственные инструменты для подсчета инструкций, но это считается неразумным ввиду ненужной сложности.

#### 2.3.2 Позднее срабатывание прерываний

Проблема заключается в том, что, пускай даже имеется возможность возможность вызвать прерывание в нужный момент на нужной инструкции на основе сказанного выше, само прерывание не сработает в этот же момент.  Чтобы это компенсировать, во время повторного воспроизведения прерывание пробрасывается несколько раз и раньше, чем RCB дойдет до нужной отметки. Когда прерывание выполнилось, ставится временная точка останова на значение в программном счетчике, в котором сработало прерывание во время записи. Далее, программа просто последовательно исполняется до тех пор, пока не дойдет до точки останова. Потом сопоставляются регистры, состояние стека и RCB, и если все совпадает, то программа продолжает исполнение после входа в прерывание.

### 2.4 Разделяемая память

Благодаря концепции одного потока в единицу времени, гонок в разделяемой памяти удается избежать. Но что, если в данный момент в системе работает системное приложение, о котором не знает RR, и которое предположительно могло бы писать в ту же память, что используется наблюдаемым потоком? Очевидно, это вызовет гонку данных с ним. К счастью, в Linux таких приложений не очень много. Это разделение памяти с демоном PulseAudio, X server, графическими драйверами ядра или графической карты и системные вызовы *vdso*. Для первых трех RR банально отключает поддержку разделяемой памяти. В случае системных вызовов решение заключается в подмене системного вызова на свой собственный, который не изменяется память наблюдаемого потока.

В случае несистемных приложений, разделяющих память с наблюдаемым потоком, можно просто расширить область отладки для RR, то есть расширить группу процессов теми, что исполняют данное стороннее приложение.

### 2.5 Недетерминированные инструкции

В целом, проблема схожа с недетерминированными системными вызовами и метод решения также относительно схож. Для точного воспроизведения RR необходимо, чтобы любая инструкция с потенциально вариативным результатом была учтена и «зафиксирована» (recorded), а затем «эмулирована» или корректно воспроизведена с тем же результатом.

Рассмотрим примеры недетерминированных инструкций и подходы к их обработке:

- *RDTSC* — инструкция, считывающая значение счетчика циклов или тактовых сигналов процессора (time stamp counter) с момента его включения. Проблема в том, что этот счетчик монотонен и растет непрерывно, и при повторном воспроизведении отлаживаемой программы важно использовать то же значение, что было при записи. Для решения  на современных Intel-процессорах можно настроить, чтобы RDTSC генерировала прерывание, и перехватить её в пользовательском пространстве (через *prctl*). RR эмулирует RDTSC, записывая фактическое значение на этапе записи и подставляя то же самое значение на этапе воспроизведения.

- *RDRAND*, которая генерирует произвольное число. На практике выяснилось, что в стандартных библиотеках зафиксировано малое ее использование, поэтому достаточно, чтобы RR подменил все вызовы этой инструкции на записанные числа.

- *CPUID* выдаёт информацию о процессоре, числе ядер, поддерживаемых инструкциях и т.д. Часть возвращаемых данных может меняться в зависимости от того, на каком ядре идёт выполнение, что делает ее недетерминированной (перемещение между ядрами может влиять не только на некоторые значения инструкции, но и даже на исполнение части кода в *glibc*). В RR используется строгая привязка потоков к одному CPU (через *sched_setaffinity*), и при возможности — включается «CPUID faulting», чтобы перехватывать все CPUID вызовы и эмулировать их, возвращая стабильный набор флагов и характеристик.

Также как вариант рассматривался стандартный подход через вызов trap прерывания к RR, который подменяет значения описанных выше инструкций на нужные или просто маскирует особенности в виде поддержки каких-либо инструкций. Для CPUID в целом подходы не сильно отличаются.

#### 2.5.1 Транзакции

В современных процессорах на базе Intel поддерживаются два типа транзакций: *"Restricted Transactional Memory”* *(“RTM”)* и *“Hardware Lock Elision”* *(“HLE”)*. Первый поддерживает явное задание транзакции через *XBEGIN* *XEND* соответственно в начале и конце, в то время как второй поддерживает неявные транзакции через добавление *XACQUIRE* и *XRELEASE*, чтобы захватить и отпустить блокировку, которая при этом еще и обратно совместимая. Разница еще в том, что у первого можно явно задать, что выполнить при отказе транзакции. Для второго же ввиду обратной совместимости при отсутствии поддержки HLE ключевые слова просто игнорируются или, если поддерживаются, в случае отказа транзакции произойдет откат на проблемную операцию, которая просто будет выполнена как блокирующая.

RTM недетерминирована из-за того, что ее аппаратная транзакция может быть разорвана из-за прерываний или состояния кэшей процессора или по иным причинам. Выяснилось, что она используется лишь в библиотеке *pthreads*, поэтому RR динамически изменяет эту библиотеку так, что она не использует RTM. В будущем заместо этого подхода будет использоваться маскировка RTM через feature bit в *CPUID*, который будет определять, что RTM поддерживается или нет.

HLE недетерминирован, потому что внутри него для RCB считаются условные ветвления даже после сброса транзакции. Для него оба подхода выше не подойдут ввиду обратной совместимости. Однако, Intel предоставляет возможность сказать процессору, чтобы внутри RCB считались условные лишь ветвления после успешных транзакций (*IN_TXCP*, бит в моделезависимом регистре), однако тут тоже есть проблема. Представим, что надо вызвать прерывание после срабатывания N ветвлений. Если N-ное ветвление происходит внутри транзакции, прерывание срабатывает, транзакция сбрасывается и счетчик, соответственно, тоже сбрасывается, и все по новой. Решение в том, чтобы использовать RCB без IN_TXCP чтобы вызвать прерывание, и RCB с IN_TXCP чтобы посчитать корректное количество условных ветвлений. Это означает, что прерывание может возникнуть раньше, но это не проблема, поскольку RR именно так и вызывает прерывания

### 2.6 Экономия места трэйса

Простое копирование необходимых бинарников, исполняемых файлов и библиотек быстро раздует память, поэтому вместо этого создаются жесткие ссылки на все необходимые файлы.

Также, современные файловые системы (*XFS*) поддерживают механизм COW (*copy-on-write*). RR просто клонирует нужные области в каталоге трэйса (записанного наблюдаемого потока), не тратя почти места и времени. Фактическое копирование произойдёт лишь в случае модификации исходного файла.

RR сжимает все остальные данные трэйса при помощи zlib (в deflate моде).

## 3. Внутри-процессный перехват системных вызовов

Подход, описанный выше, работает, но с большими накладными расходами. Ключевая проблема в количестве смен контекста (4), которыми оперирует RR через ptrace, а именно два блокирующих уведомления от ptrace, каждый требует смену контекста в RR и обратно. Чтобы уменьшить количество смен контекстов, в момент запуска RR размещает в адресном пространстве каждого наблюдаемого процесса специальную библиотеку (interception library), которая перехватывает системные вызовы без срабатывания ptrace прерываний, и записывает результаты срабатываний в разделяемую с RR буфер, который сбрасывает содержимое в наблюдаемый поток каждый раз, как получает синхронное уведомление.

Рассмотрим проблемы, которые возникли с этим концептом.

### 3.1 Перехват системных вызовов

Можно через динамическую линковку вставить функции "обертки" заместо функций из библиотеки C, которые выполняют системные вызовы, но на практике этого недостаточно: в приложении системные вызовы могут делаться напрямую или вообще могут использовать собственные механизмы загрузки перед исполнением.

Вместо этого, когда срабатывает системное уведомление от ptrace (наблюдаемый поток дошел до системного вызова), RR пытается заменить этот вызов на инструкцию *call* в специальную функцию (stub) interception library, и при этом, на x86_64 придется в диапазоне ±2GB (из-за размера инструкции call) ставить дополнительно трамплины в stub. Далее, stub решает, какой режим выполнения системного вызова выбрать: использовать обертку из interception library (непросматриваемый системный вызов) или провести стандартную процедуру с выполнением системного вызова через ptrace (просматриваемый системный вызов).

### 3.2 Выборочное уведомление о системных вызовах

Хочется, чтобы ptrace не порождал трапы для отдельных системных вызовов из-за возникновений ненужных смен контекста. Для этого можно использовать *seccomp-bpf* фильтрацию. Процесс применяет этот фильтр, выраженный в байт-коде, к другому процессу; далее, перед системным вызовом, ядро запускает фильтр, передавая туда значения регистров, программный счетчик и т.п, получает результат в виде либо разрешения на вызов, либо отказа с ошибкой (записанной в *errno*), либо убивает процесс, либо производит ptrace trap. Накладные расходы при этом незначительные поскольку это нативный и уже скомпилированный код, выполняемый в ядре.

Как было сказано выше, в каждом наблюдаемом процессе RR добавляет "RR Page", содержащую в себе специальную "непросматриваемую" инструкцию. Фильтрация применяется к каждому наблюдаемому процессу, и каждый системный вызов проходит через нее, кроме этих специальных инструкций.

### 3.3 Выявление блокирующих системных вызовов

В течение исполнения на некотором потоке вызывался системный вызов, и бывает так, что он может зависнуть (чтение потока ввода до определенного момента). Нужно как-то обработать эту ситуацию. В случае непросматриваемого блокирующего системного вызова (то есть RR не узнает о том, что наблюдаемый им поток начал его исполнение), он просто зависнет. Поэтому важно получить уведомление о таком вызове (как минимум понять, что он блокирующий), чтобы оперативно сместить выполнение на другой наблюдаемый поток.

Для этого в Linux есть система мониторинга *perf* с событием *PERF_COUNT_SW_CONTEXT_SWITCHES*. Как только ядро забрало квант вычисления у потока, оно поднимает это событие. Interception library мониторит это событие из-под наблюдаемого потока и уведомляет RR (через ptrace), как только оно было поднято. Чтобы избежать ложных пробуждений (spurious wake-up, возникающий когда текущий поток израсходовал все свое время, которое ему выделил планировщик), событие в основном отключено и включается лишь тогда, когда наблюдаемый поток исполняет непросматриваемый системный вызов. Также, между включением и отключением мониторинга этого события могут возникнуть ложные пробуждения. В такой ситуации надо внимательно проверять состояние наблюдаемого потока.

### 3.4 Обработка сигналов

Что, если во время выполнения кода в interception library возникнет асинхронный сигнал или смена контекста? Сама по себе interception library нереентрабельна, поэтому могу возникнуть проблемы. RR, когда узнает от таких сигналах, откладывает их, а заместо их обработки говорит вызвать "глупый" системный вызов (нужный затем, чтобы показать RR, что есть отложенный системного вызов), и возобновляет работу без доставки сигнала. После корректной записи текущего системного вызова начинает воспроизводиться отложенный заместо этого "глупого" системного вызова.

Но для такого подхода есть краевые случаи:

- Системный вызов *sigprocmask*. Например, сигнал во время исполнения кода в interception library доходит перед тем, как маска была выставлена. Если его отложить, то этот сигнал просто проигнорируется. Следовательно, RR ставит точку останова на sigprocmask (по умолчанию он непросматриваемый). Если на данный момент есть отложенный сигнал, Interception library делает его просматриваемым через ptrace, подменяет вызов, так чтобы sigprocmask завершился с ошибкой *EAGAIN* (это дает RR возможность заместо кода из interception library обработать стандартный просматриваемый системный вызов), обрабатывает отложенный вызов, возвращается к sigprocmask и исполняет его корректно.

- Представим, обработчик сигнала S1 настроен блокировать обработку сигнала S2 внутри себя и есть два наблюдаемых потока Т1 и Т2. Может возникнуть такая ситуация: S1 доставлен потоку Т1, при этом Т1 исполняет код interception library, значит, S1 откладывается; далее, приходит S2 в Т1. Из условия  допустимо, что S2 никогда не будет отработан, что некорректно, поскольку во время стандартного исполнения программы S1 мог быть доставлен в Т2. По имеющимся данным из пользовательского пространства нельзя сказать, куда конкретно будет отправлен сигнал если он будет блокирован в каком-то потоке, потому что нельзя выявить разницу между сигналом, отправленным потоку или процессу. Следовательно, важно не доставлять сигнал потоку если нет уверенности, сможет ли этот поток обработать сигнал. Значит, не может быть больше одного отложенного сигнала в одном потоке.

### 3.5 Прерывание системных вызовов

Непросматриваемый системный вызов может быть прерван сигналом, и обработчик сигнала должен начать исполняться. Здесь есть сложность: в то время как сейчас исполняется код interception library, обработчиком может быть код приложения , который может еще раз войти в interception library, что недопустимо. Решение в том, чтобы на каждый поток иметь thread-local "locked" флаг, говороящий о том, что любой перехваченный системный вызов не должен становиться непросматриваемым до тех пор, пока текущий поток не выйдет из обработчика ошибок.

### 3.6 Thread-Local хранилище

Interception library необходимо порой брать данные из локальных для потоков хранилищ. Иногда подобные хранилища могут быть некорректно настроены (например, когда Chromium использует чистый вызов clone для создания потока). Чтобы решить эту проблему, RR выделяет специальную страницу памяти по фиксированному адресу для хранения thread-local данных. При каждом переключении контекста RR копирует значения из этой страницы для старого потока и загружает данные для нового потока. Таким образом, interception library всегда имеет доступ к корректным потоковым данным. Это также упрощает возможность настройки всех системных вызовов, инициализирующих потоки и TLS.

### 3.7 Обработка стэка

На x86-64 все нерекурсивные функции могут иметь так называемую "красную зону" на стэке (размером 128 байт), чтобы избежать переинициализации стэка и куда могут записываться некоторые временные данные. Interception library не должна каким-то образом взаимодействовать или доходить до этой области, потому что это вызовет ложное переполнение стека (ложное, потому что пользователь не знает о том, что в программу встроена непонятно какая библиотека, которая еще и к тому же заставила его программу сломаться). Поэтому трамплины также меняют стэк, аллоцированный как часть рабочего буфера данного наблюдаемого потока.

### 3.8 Обработка повторного воспроизведения

Из подхода, описанного выше, известно, что во время записи надо сохранять все результаты выполнения системного вызова в некий буфер (trace), а потом при повторном воспроизведении из него доставать нужные данные и подменять их в после перехвата системного вызова (либо же в interception library) в наблюдаемом потоке. Из-за того, что interception library есть тоже часть логики, то она не может позволить себе выполнять такие действия. Следовательно, interception library говорит, чтобы все результаты системного вызова сразу перенаправлялись в trace, и уже потом, после исполнения системного вызова, все данные копируются в основной буфер вывода для наблюдаемого потока. Непросматриваемые системные вызовы просто заменяются ничем (no-op), так как уже имеются нужные данные, которые надо просто перекопировать в основной буфер вывода.

Для того чтобы условные переходы и RCB-счётчики не расходились между записью и повторным воспроизведением, используются условные move-инструкции. Они зависят от глобальной переменной RR, которая определяет, находится ли система в режиме записи или воспроизведения, позволяя минимизировать любые расхождения.

Если системный вызов имеет параметры, которые и передаются в ядро, и изменяются им (например, буферы ввода-вывода), во время записи буфер копируется в trace buffer, а затем — обратно. При повторном воспроизведении эта копия превращается в no-op, чтобы не перезаписать уже сохранённый результат.

### 3.9 Оптимизация чтения с помощью клонирования файлов

Если и трэйс, и входные данные находятся в одной файловой системе, поддерживающей копирование при записи, то в коде interception library системного вызова *read* реализуется клонирование блоков в "cloned-data" трэйс файл, свой для каждого наблюдаемого потока.

Оптимизация работает так, что происходит некое логическое связывание блоков входного файла с файлом трэйса и только потом чтение входных данных из основного входного файла. Может возникнуть гонка данных (между записью и повторным воспроизведением кто-то поменяет содержимое входного файла), но в таком случае в Linux это вызовет некий мешанину из старого и нового содержимого, что есть некорректно. Избежать гонки можно через чтение из клонированного файла, а не основного, но это уберет оптимизацию "readahead" у Linux, так как данные в клонированном файле недоступны до тех пор, пока они не понадобились.

## 4. Результаты

Далее оценивается производительность и RR демонстрирует, что запись и воспроизведение с помощью RR имеют приемлемые накладные расходы для ряда реальных сценариев. Авторы выбрали несколько рабочих нагрузок, включая операции с файлами (cp), сборку (make), вычислительные тесты (octane), тестирование HTML-форм (htmltest) и UDP-тесты Samba (sambatest). Основные результаты можно резюмировать следующим образом:

1. Производительность и оверхед:

    - Запись: За исключением сборки make, где доминирует большое число короткоживущих процессов и форков, запись с RR замедляет выполнение в среднем менее чем в 2 раза по сравнению с базовым запуском.

    - Воспроизведение: Во многих случаях повторное воспроизведение оказывается даже быстрее нативного выполнения, поскольку реальные системные вызовы не выполняются, а лишь эмулируются на основе ранее записанных данных.

    - Сравнение с динамической инструментализацией: Эксперименты с использованием DynamoRio (null tool) показывают, что минимальный оверхед динамической инструментализации может быть сопоставим с RR на некоторых нагрузках, но при этом для сложных сценариев (например, при работе с динамически генерируемым кодом) RR значительно выигрывает по эффективности.

2. Влияние оптимизаций:

    - Перехват системных вызовов: Использование внутри-процессных перехватов существенно снижает количество дорогостоящих ptrace-переключений. При отключении этой оптимизации наблюдается резкое увеличение оверхеда, что подчёркивает её критическую роль.

    - Блоковое клонирование: Для операций, связанных с чтением больших блоков данных, механизм клонирования позволяет избежать фактического копирования содержимого файлов в трасс, что приводит к значительному сокращению как времени записи, так и размера результирующего лога.

3. Использование дискового пространства и память:

    - Размер трэйса: Благодаря применению жёстких ссылок, блоков клонирования и сжатию с помощью deflate, объем данных, сохраняемых в трэйсе, остаётся на приемлемом уровне даже для длительных сессий записи.

    - Пиковое использование памяти: Замеры памяти показывают, что даже при дополнительных затратах, связанных с перехватом системных вызовов и поддержкой рабочих буферов, общий объем памяти (по PSS) остается в пределах, приемлемых для современных десктопных систем. При этом ограничение работы на одном ядре, необходимое для обеспечения детерминированности, дополнительно снижает одновременное потребление памяти при большом количестве процессов.

Таким образом, эксперименты в секции 4 показывают, что RR способен обеспечить детерминированную запись и воспроизведение с приемлемым оверхедом, а также демонстрируют, что благодаря ряду оптимизаций можно свести к минимуму проблемы, связанные с объемом трэйса и использованием ресурсов системы.

## 5 Общий вывод

1. Какие уроки были извлечены в ходе работы:

    - Деплой и простота использования:

        RR разработана так, чтобы быть применимой в реальных пользовательских средах, не требуя специальных привилегий, модификаций ядра или использования виртуальных машин. Это значительно снижает порог входа для отладчиков и аналитиков, позволяя внедрять новую технологию без существенных изменений в существующей инфраструктуре.

    - Надёжность и стабильность:

        Система сконструирована с акцентом на высокую стабильность – важно, чтобы RR работала корректно в течение длительных сессий отладки, даже если не все приложения идеально подходят для такого подхода. Отказ от поддержки чрезмерной параллельности и упор на обработку низкопараллельных сценариев способствуют достижению надежности, позволяя системе работать «100 % времени на 90 % приложений».

    - Управление инженерной сложностью:

        Фокус на ключевых сценариях применения (например, отладке сложных, но детерминированных ошибок) позволил авторам избежать избыточной сложности. Разумное ограничение функционала и отказ от попыток охватить все возможные сценарии приводят к более устойчивой и поддерживаемой реализации.

2. Перспективы и направления для будущей работы:

    - Расширение аппаратной поддержки и улучшение эмуляции:

        Авторы отмечают, что дальнейшее развитие RR должно быть ориентировано на более тонкую виртуализацию аппаратных счётчиков и совершенствование механизмов эмуляции недетерминированных инструкций (например, RDRAND или транзакционных инструкций). Это позволит повысить точность фиксации состояний при записи и сделать воспроизведение ещё более достоверным.

    - Интеграция с современными ОС и виртуальными средами:

        Важным направлением является улучшение совместимости с новыми версиями Linux, а также обеспечение корректной работы RR в виртуализованных и облачных средах. Решение проблем, связанных с виртуализацией аппаратных счётчиков, и адаптация к изменяющемуся пользовательско-ядерному интерфейсу остаются актуальными задачами.

    - Расширение функциональности отладочных инструментов:

        RR уже доказала свою эффективность в построении обратного исполнения (reverse debugging) и других средств анализа. Будущие исследования могут быть направлены на разработку дополнительных инструментов, использующих RR как базу, для более глубокого анализа поведения приложений, создания точек останова и поддержки интерактивной отладки в режиме реального времени.

Статья демонстрирует, что запись и повторное воспроизведение исполнения пользовательских процессов с низким оверхедом возможны даже без модификации ядра или использования виртуальных машин. Авторы предлагают систему RR, которая с помощью комбинации современных возможностей аппаратного обеспечения (например, детерминированного счётчика «Retired Conditional Branches» на Intel‑процессорах) и возможностей Linux (таких как seccomp‑bpf, PERF‑события, поддержка copy-on‑write файловых систем) способна фиксировать все источники недетерминизма на границе между пользовательским пространством и ядром. Это позволяет точно записывать состояние программы, включая все входные данные и результаты системных вызовов, и воспроизводить их детерминированно, что особенно важно при отладке сложных и трудно воспроизводимых ошибок.

